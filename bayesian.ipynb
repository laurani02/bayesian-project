{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-project task with linear regression in multidimensional input space and batch learning ##\n",
    "\n",
    "Authors: Laura Nilsson & Leo Svanemar\n",
    "Date: 2023-05-22\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot data & divide into test and training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1 and x2 are limited to [-1, 1]x[-1, 1] with a step of 0.05\n",
    "x1 = np.arange(-1, 1.05, 0.05)\n",
    "x2 = np.arange(-1, 1.05, 0.05)\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "\n",
    "# assumed values of weight vector\n",
    "w = [0, 1.5, -0.8]\n",
    "\n",
    "# values of data noice, σ2 ∈ {0.2, 0.4, 0.6}\n",
    "sigma_values_squared = [0.2, 0.4, 0.6]\n",
    "\n",
    "# set seed for reproducibility, and 42 is the answer to everything :)\n",
    "np.random.seed(42) \n",
    "\n",
    "# plot the data\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# divide into training data and test data\n",
    "training_data = np.logical_or(np.abs(X1) <= 0.3, np.abs(X2) <= 0.3)\n",
    "test_data = np.logical_or(np.abs(X1) > 0.3, np.abs(X2) > 0.3)\n",
    "\n",
    "# train on the training data (obviously)\n",
    "for sigma in sigma_values_squared:\n",
    "    t = w[0] + w[1] * X1 + w[2] * X2 + np.random.normal(0, sigma, size=X1.shape)\n",
    "    ax.scatter()\n",
    "    ax.scatter( t[training_data])\n",
    "    #ax.scatter(X1[training_data], X2[training_data], t[training_data], label=f\"Train σ² = {sigma}\")\n",
    "    #ax.scatter(X1[test_data], X2[test_data], t[test_data], label=f\"Test σ² = {sigma}\")\n",
    "\n",
    "# add labels and legend\n",
    "ax.set_xlabel('x1')\n",
    "#ax.set_ylabel('x2')\n",
    "#ax.set_zlabel('t')\n",
    "ax.legend()\n",
    "\n",
    "# change the view angle\n",
    "#ax.view_init(elev=30, azim=30)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# x1 and x2 are limited to [-1, 1]x[-1, 1] with a step of 0.05\n",
    "x1 = np.arange(-1, 1.05, 0.05)\n",
    "x2 = np.arange(-1, 1.05, 0.05)\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "\n",
    "# assumed values of weight vector\n",
    "w = [0, 1.5, -0.8]\n",
    "\n",
    "# values of data noise, σ² ∈ {0.2, 0.4, 0.6}\n",
    "sigma_values_squared = [0.2, 0.4, 0.6]\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(42) \n",
    "\n",
    "# plot the data\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# divide into training data and test data\n",
    "training_data = np.logical_or(np.abs(X1) <= 0.3, np.abs(X2) <= 0.3)\n",
    "test_data = np.logical_or(np.abs(X1) > 0.3, np.abs(X2) > 0.3)\n",
    "\n",
    "# train on the training data\n",
    "for sigma_squared in sigma_values_squared:\n",
    "    # Generate the target values with Gaussian noise\n",
    "    t = w[0] + w[1] * X1 + w[2] * X2 + np.random.normal(0, np.sqrt(sigma_squared), size=X1.shape)\n",
    "    \n",
    "    ax.scatter(X1[training_data], X2[training_data], t[training_data], label=f\"Train σ² = {sigma_squared}\")\n",
    "    ax.scatter(X1[test_data], X2[test_data], t[test_data], label=f\"Test σ² = {sigma_squared}\")\n",
    "\n",
    "# add labels and legend\n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('x2')\n",
    "ax.set_zlabel('t')\n",
    "ax.legend()\n",
    "\n",
    "# change the view angle\n",
    "ax.view_init(elev=30, azim=30)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Fit the model using maximum likelihood principle and calculate MSE on test data\n",
    "mse_values = []\n",
    "for sigma_squared in sigma_values_squared:\n",
    "    # Generate the target values with Gaussian noise\n",
    "    t = w[0] + w[1] * X1 + w[2] * X2 + np.random.normal(0, np.sqrt(sigma_squared), size=X1.shape)\n",
    "\n",
    "    # Create boolean indices for training and test data\n",
    "    train_indices = np.logical_or(np.abs(X1) <= 0.3, np.abs(X2) <= 0.3)\n",
    "    test_indices = np.logical_or(np.abs(X1) > 0.3, np.abs(X2) > 0.3)\n",
    "\n",
    "    # Calculate the target values for training and test data\n",
    "    train_targets = t[train_indices].flatten()\n",
    "    test_targets = t[test_indices].flatten()\n",
    "\n",
    "    # Fit the model using maximum likelihood on the training data\n",
    "    X_train = np.column_stack((np.ones_like(X1[train_indices]), X1[train_indices], X2[train_indices]))\n",
    "    w_hat = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ train_targets\n",
    "\n",
    "    # Predict the test data using the estimated weights\n",
    "    X_test = np.column_stack((np.ones_like(X1[test_indices]), X1[test_indices], X2[test_indices]))\n",
    "    test_predictions = X_test @ w_hat\n",
    "    \n",
    "    # Calculate the mean squared error (MSE) on the test data\n",
    "    mse = np.mean((test_predictions - test_targets) ** 2)\n",
    "    mse_values.append(mse)\n",
    "    \n",
    "for i, sigma_squared in enumerate(sigma_values_squared):\n",
    "    print(f\"MSE (σ² = {sigma_squared}) = {mse_values[i]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# x1 and x2 are limited to [-1, 1]x[-1, 1] with a step of 0.05\n",
    "x1 = np.arange(-1, 1.05, 0.05)\n",
    "x2 = np.arange(-1, 1.05, 0.05)\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "\n",
    "# assumed values of weight vector\n",
    "w = [0, 1.5, -0.8]\n",
    "\n",
    "# values of data noise, σ² ∈ {0.2, 0.4, 0.6}\n",
    "sigma_values_squared = [0.2, 0.4, 0.6]\n",
    "\n",
    "# gaussian prior distribution\n",
    "prior_mean = np.zeros(X_train.shape[1])  # Prior mean is zero\n",
    "alpha_values = [0.7, 1.5, 3.0]  # Values of the uncertainty parameter\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(42) \n",
    "\n",
    "# plot the data\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# divide into training data and test data\n",
    "training_data = np.logical_or(np.abs(X1) <= 0.3, np.abs(X2) <= 0.3)\n",
    "test_data = np.logical_or(np.abs(X1) > 0.3, np.abs(X2) > 0.3)\n",
    "\n",
    "# train on the training data\n",
    "for sigma_squared in sigma_values_squared:\n",
    "    # Generate the target values with Gaussian noise\n",
    "    t = w[0] + w[1] * X1 + w[2] * X2 + np.random.normal(0, np.sqrt(sigma_squared), size=X1.shape)\n",
    "    \n",
    "    ax.scatter(X1[training_data], X2[training_data], t[training_data], label=f\"Train σ² = {sigma_squared}\")\n",
    "    ax.scatter(X1[test_data], X2[test_data], t[test_data], label=f\"Test σ² = {sigma_squared}\")\n",
    "\n",
    "# add labels and legend\n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('x2')\n",
    "ax.set_zlabel('t')\n",
    "ax.legend()\n",
    "\n",
    "# change the view angle\n",
    "ax.view_init(elev=0, azim=0)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Fit the model using maximum likelihood principle and calculate MSE on test data\n",
    "mse_values = []\n",
    "for sigma_squared in sigma_values_squared:\n",
    "    #print(f\"σ² = {sigma_squared}\")\n",
    "    for alpha in alpha_values:\n",
    "        #print(f\"α = {alpha}\")\n",
    "        # Generate the target values with Gaussian noise\n",
    "        t = w[0] + w[1] * X1 + w[2] * X2 + np.random.normal(0, np.sqrt(sigma_squared), size=X1.shape)\n",
    "\n",
    "        # Create boolean indices for training and test data\n",
    "        train_indices = np.logical_or(np.abs(X1) <= 0.3, np.abs(X2) <= 0.3)\n",
    "        test_indices = np.logical_or(np.abs(X1) > 0.3, np.abs(X2) > 0.3)\n",
    "\n",
    "        # Calculate the target values for training and test data\n",
    "        train_targets = t[train_indices].flatten()\n",
    "        test_targets = t[test_indices].flatten()\n",
    "\n",
    "        # Fit the model using maximum likelihood on the training data\n",
    "        X_train = np.column_stack((np.ones_like(X1[train_indices]), X1[train_indices], X2[train_indices]))\n",
    "        w_hat = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ train_targets\n",
    "        \n",
    "        # calculate the posterior distribution w/ Bayesian interference\n",
    "        posterior_precision = np.eye(X_train.shape[1]) * alpha + X_train.T @ X_train / sigma_squared\n",
    "        posterior_covariance = np.linalg.inv(posterior_precision)\n",
    "        posterior_mean = posterior_covariance @ (X_train.T @ train_targets) / sigma_squared\n",
    "\n",
    "\n",
    "        # Predict the test data using the estimated weights\n",
    "        X_test = np.column_stack((np.ones_like(X1[test_indices]), X1[test_indices], X2[test_indices]))\n",
    "        test_predictions = X_test @ posterior_mean\n",
    "        \n",
    "        # Calculate the mean squared error (MSE) on the test data\n",
    "        #print((test_predictions - test_targets) ** 2)\n",
    "        mse = np.mean((test_predictions - test_targets) ** 2)\n",
    "        mse_values.append(mse)\n",
    "        print(f\"MSE (σ² = {sigma_squared}, α = {alpha}) = {mse}\")\n",
    "    \n",
    "#for i, sigma_squared in enumerate(sigma_values_squared):\n",
    "#    print(f\"MSE (σ² = {sigma_squared}) = {mse_values[i]}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5\n",
    "\n",
    "A comparsion between the frequentist approach MLE and the Bayesian  can be made by comparing the MSE. \n",
    "\n",
    "\n",
    "#### MLE\n",
    "\n",
    "MSE (σ² = 0.2) = 0.1971901417530678  \n",
    "MSE (σ² = 0.4) = 0.4491506427837673  \n",
    "MSE (σ² = 0.6) = 0.556552760694397  \n",
    "\n",
    "#### Bayesian\n",
    "\n",
    "MSE (σ² = 0.2, α = 0.7) = 0.1971494682678761  \n",
    "MSE (σ² = 0.2, α = 1.5) = 0.22452106396164226  \n",
    "MSE (σ² = 0.2, α = 3.0) = 0.18572319863608935  \n",
    "MSE (σ² = 0.4, α = 0.7) = 0.3981424796062423  \n",
    "MSE (σ² = 0.4, α = 1.5) = 0.400761638968057  \n",
    "MSE (σ² = 0.4, α = 3.0) = 0.4069250692467875  \n",
    "MSE (σ² = 0.6, α = 0.7) = 0.6091655155964246  \n",
    "MSE (σ² = 0.6, α = 1.5) = 0.6000864367300098  \n",
    "MSE (σ² = 0.6, α = 3.0) = 0.6091084391212728  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# x1 and x2 are limited to [-1, 1]x[-1, 1] with a step of 0.05\n",
    "x1 = np.arange(-1, 1.05, 0.05)\n",
    "x2 = np.arange(-1, 1.05, 0.05)\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "\n",
    "# assumed values of weight vector\n",
    "w = [0, 1.5, -0.8]\n",
    "\n",
    "# values of data noise, σ² ∈ {0.2, 0.4, 0.6}\n",
    "sigma_values_squared = [0.2, 0.4, 0.6]\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# divide into training data and test data\n",
    "training_data = np.logical_or(np.abs(X1) <= 0.3, np.abs(X2) <= 0.3)\n",
    "test_data = np.logical_or(np.abs(X1) > 0.3, np.abs(X2) > 0.3)\n",
    "\n",
    "# Perform model fitting and calculate predictions for each approach\n",
    "frequentist_predictions = []\n",
    "bayesian_predictions = []\n",
    "alpha_values = [0.7, 1.5, 3.0]\n",
    "\n",
    "for sigma_squared in sigma_values_squared:\n",
    "    for alpha in alpha_values:\n",
    "        # Generate the target values with Gaussian noise\n",
    "        t = w[0] + w[1] * X1 + w[2] * X2 + np.random.normal(0, np.sqrt(sigma_squared), size=X1.shape)\n",
    "\n",
    "        # Create boolean indices for training and test data\n",
    "        train_indices = np.logical_or(np.abs(X1) <= 0.3, np.abs(X2) <= 0.3)\n",
    "        test_indices = np.logical_or(np.abs(X1) > 0.3, np.abs(X2) > 0.3)\n",
    "\n",
    "        # Calculate the target values for training and test data\n",
    "        train_targets = t[train_indices].flatten()\n",
    "        test_targets = t[test_indices].flatten()\n",
    "\n",
    "        # Fit the model using maximum likelihood on the training data\n",
    "        X_train = np.column_stack((np.ones_like(X1[train_indices]), X1[train_indices], X2[train_indices]))\n",
    "        w_hat = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ train_targets\n",
    "\n",
    "        # Predict the test data using the estimated weights\n",
    "        X_test = np.column_stack((np.ones_like(X1[test_indices]), X1[test_indices], X2[test_indices]))\n",
    "        frequentist_test_predictions = X_test @ w_hat\n",
    "\n",
    "        # Perform Bayesian inference\n",
    "        posterior_precision = np.eye(X_train.shape[1]) * alpha + X_train.T @ X_train / sigma_squared\n",
    "        posterior_covariance = np.linalg.inv(posterior_precision)\n",
    "        posterior_mean = posterior_covariance @ (X_train.T @ train_targets) / sigma_squared\n",
    "\n",
    "        # Predict the test data using the posterior mean of the weights\n",
    "        bayesian_test_predictions = X_test @ posterior_mean\n",
    "\n",
    "        # Store the predictions for analysis\n",
    "        frequentist_predictions.append(frequentist_test_predictions)\n",
    "        bayesian_predictions.append(bayesian_test_predictions)\n",
    "\n",
    "# Calculate standard deviation and mean of the predictions for each approach\n",
    "frequentist_std = [np.std(predictions) for predictions in frequentist_predictions]\n",
    "frequentist_mean = [np.mean(predictions) for predictions in frequentist_predictions]\n",
    "\n",
    "bayesian_std = [np.std(predictions) for predictions in bayesian_predictions]\n",
    "bayesian_mean = [np.mean(predictions) for predictions in bayesian_predictions]\n",
    "\n",
    "# Print the standard deviation and mean for each approach\n",
    "'''\n",
    "for i in range(len(sigma_values_squared)):\n",
    "    for j in range(len(alpha_values)):\n",
    "        print(f\"Sigma^2 = {sigma_values_squared[i]}, Alpha = {alpha_values[j]}\")\n",
    "        print(\"Frequentist - Std:\", frequentist_std[i*len(alpha_values)+j])\n",
    "        print(\"Frequentist - Mean:\", frequentist_mean[i*len(alpha_values)+j])\n",
    "        print(\"Bayesian - Std:\", bayesian_std[i*len(alpha_values)+j])\n",
    "        print(\"Bayesian - Mean:\", bayesian_mean[i*len(alpha_values)+j])\n",
    "        print()\n",
    "        '''\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "Sigma^2 = 0.2, Alpha = 0.7  \n",
    "Frequentist - Std: 1.0415402095216657  \n",
    "Frequentist - Mean: 0.029277048894942783  \n",
    "Bayesian - Std: 1.0407432265081926  \n",
    "Bayesian - Mean: 0.029264653171258218  \n",
    "\n",
    "Sigma^2 = 0.2, Alpha = 1.5  \n",
    "Frequentist - Std: 1.0776108020201185  \n",
    "Frequentist - Mean: 0.015004771205655066  \n",
    "Bayesian - Std: 1.0758455310388912  \n",
    "Bayesian - Mean: 0.01498431885304853  \n",
    "\n",
    "Sigma^2 = 0.2, Alpha = 3.0  \n",
    "Frequentist - Std: 1.0702834088950497  \n",
    "Frequentist - Mean: -0.02684921624949407  \n",
    "Bayesian - Std: 1.0667830726130483  \n",
    "Bayesian - Mean: -0.026857178186670293  \n",
    "\n",
    "Sigma^2 = 0.4, Alpha = 0.7  \n",
    "Frequentist - Std: 1.106565142489633  \n",
    "Frequentist - Mean: -0.023208521303072552  \n",
    "Bayesian - Std: 1.1048732936631402  \n",
    "Bayesian - Mean: -0.023214406001312315  \n",
    "\n",
    "Sigma^2 = 0.4, Alpha = 1.5  \n",
    "Frequentist - Std: 1.065705345433871  \n",
    "Frequentist - Mean: 0.005538601215642742  \n",
    "Bayesian - Std: 1.0622195101496643  \n",
    "Bayesian - Mean: 0.005502790301335658  \n",
    "\n",
    "Sigma^2 = 0.4, Alpha = 3.0  \n",
    "Frequentist - Std: 1.0143156039690722  \n",
    "Frequentist - Mean: -0.0004059187270128801  \n",
    "Bayesian - Std: 1.0077022567743306  \n",
    "Bayesian - Mean: -0.0004560067631212476  \n",
    "\n",
    "Sigma^2 = 0.6, Alpha = 0.7  \n",
    "Frequentist - Std: 0.9866602056256505  \n",
    "Frequentist - Mean: 0.0007077539817422582  \n",
    "Bayesian - Std: 0.9843989345876711  \n",
    "Bayesian - Mean: 0.0006871280427911734  \n",
    "\n",
    "Sigma^2 = 0.6, Alpha = 1.5  \n",
    "Frequentist - Std: 1.0192576516488154  \n",
    "Frequentist - Mean: 0.004905221804971727  \n",
    "Bayesian - Std: 1.0142651917051924  \n",
    "Bayesian - Mean: 0.004859680921884332  \n",
    "\n",
    "Sigma^2 = 0.6, Alpha = 3.0  \n",
    "Frequentist - Std: 1.0497923016177393  \n",
    "Frequentist - Mean: 0.026820761932184155  \n",
    "Bayesian - Std: 1.039557899736239  \n",
    "Bayesian - Mean: 0.02668229540784155  \n",
    "\n",
    "#### Analysis\n",
    "\n",
    "The **standard deviation** represents the spread or uncertainty of the predictions. In general, the standard deviation tends to be similar between the frequentist and Bayesian approaches for each combination of sigma_squared and alpha.\n",
    "\n",
    "The **mean** represents the average prediction and those values differ slightly between the frequentist and Bayesian approaches for each combination of sigma_squared and alpha.\n",
    "\n",
    "Overall, the differences between the frequentist and Bayesian approaches in terms of standard deviation and mean are relatively small. The Bayesian approach accounts for uncertainty by incorporating the prior distribution over the weight parameters, while the frequentist approach relies solely on the maximum likelihood estimation. However, in this specific scenario, the differences in predictive performance between the two approaches appear to be minimal."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6\n",
    "For the Bayesian approach, generate predictions also for the training data and compare the variance of the predicitions (uncertainty) between the training and test data samples. You can only do that for the Bayesian framework as it outputs the Gaussian distribution with the mean (first statistical moment) and standard deviation (second statistical moment). How does uncertainty (standard deviation) and the quality (mean) of these 2\n",
    "predictions change with varying α and σ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# x1 and x2 are limited to [-1, 1]x[-1, 1] with a step of 0.05\n",
    "x1 = np.arange(-1, 1.05, 0.05)\n",
    "x2 = np.arange(-1, 1.05, 0.05)\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "\n",
    "# assumed values of weight vector\n",
    "w = [0, 1.5, -0.8]\n",
    "\n",
    "# values of data noise, σ² ∈ {0.2, 0.4, 0.6}\n",
    "sigma_values_squared = [0.2, 0.4, 0.6]\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# divide into training data and test data\n",
    "training_data = np.logical_or(np.abs(X1) <= 0.3, np.abs(X2) <= 0.3)\n",
    "test_data = np.logical_or(np.abs(X1) > 0.3, np.abs(X2) > 0.3)\n",
    "\n",
    "# Perform model fitting and calculate predictions for each approach\n",
    "bayesian_train_predictions = []\n",
    "bayesian_test_predictions = []\n",
    "alpha_values = [0.7, 1.5, 3.0]\n",
    "\n",
    "for sigma_squared in sigma_values_squared:\n",
    "    for alpha in alpha_values:\n",
    "        # Generate the target values with Gaussian noise\n",
    "        t = w[0] + w[1] * X1 + w[2] * X2 + np.random.normal(0, np.sqrt(sigma_squared), size=X1.shape)\n",
    "\n",
    "        # Create boolean indices for training and test data\n",
    "        train_indices = np.logical_or(np.abs(X1) <= 0.3, np.abs(X2) <= 0.3)\n",
    "        test_indices = np.logical_or(np.abs(X1) > 0.3, np.abs(X2) > 0.3)\n",
    "\n",
    "        # Calculate the target values for training and test data\n",
    "        train_targets = t[train_indices].flatten()\n",
    "        test_targets = t[test_indices].flatten()\n",
    "\n",
    "        # Fit the model using maximum likelihood on the training data\n",
    "        X_train = np.column_stack((np.ones_like(X1[train_indices]), X1[train_indices], X2[train_indices]))\n",
    "        w_hat = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ train_targets\n",
    "\n",
    "        # Perform Bayesian inference\n",
    "        posterior_precision = np.eye(X_train.shape[1]) * alpha + X_train.T @ X_train / sigma_squared\n",
    "        posterior_covariance = np.linalg.inv(posterior_precision)\n",
    "        posterior_mean = posterior_covariance @ (X_train.T @ train_targets) / sigma_squared\n",
    "\n",
    "        # Predict the training and test data using the posterior distribution\n",
    "        bayesian_train_predictions.append(X_train @ posterior_mean)\n",
    "        bayesian_test_predictions.append(X_test @ posterior_mean)\n",
    "\n",
    "# Calculate standard deviation and mean of the predictions for each approach\n",
    "bayesian_train_std = [np.std(predictions) for predictions in bayesian_train_predictions]\n",
    "bayesian_train_mean = [np.mean(predictions) for predictions in bayesian_train_predictions]\n",
    "\n",
    "bayesian_test_std = [np.std(predictions) for predictions in bayesian_test_predictions]\n",
    "bayesian_test_mean = [np.mean(predictions) for predictions in bayesian_test_predictions]\n",
    "\n",
    "# Print the standard deviation and mean for training and test data\n",
    "\n",
    "for i in range(len(sigma_values_squared)):\n",
    "    for j in range(len(alpha_values)):\n",
    "        print(f\"Sigma^2 = {sigma_values_squared[i]}, Alpha = {alpha_values[j]}\")\n",
    "        print(\"Bayesian - Training Data\")\n",
    "        print(\"Std:\", bayesian_train_std[i * len(alpha_values) + j])\n",
    "        print(\"Mean:\", bayesian_train_mean[i * len(alpha_values) + j])\n",
    "        print(\"Bayesian - Test Data\")\n",
    "        print(\"Std:\", bayesian_test_std[i * len(alpha_values) + j])\n",
    "        print(\"Mean:\", bayesian_test_mean[i * len(alpha_values) + j])\n",
    "        print()\n",
    "        \n",
    "\n",
    "\n",
    "#Observations:\n",
    "\n",
    "#1. Standard Deviation (Uncertainty): The standard deviation represents the uncertainty or variance of the predictions. By comparing the standard deviation values between the training and test data, you can observe how the uncertainty changes.\n",
    "\n",
    "#2. Mean: The mean represents the average prediction. You can also compare the mean values between the training and test data to analyze how the predictions differ in terms of quality.\n",
    "\n",
    "#By varying `alpha` and `sigma_squared`, you can examine how the uncertainty (standard deviation) and the quality (mean) of the predictions change in the Bayesian approach for both the training and test data.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "\n",
    "The **standard deviation** represents the spread or uncertainty of the predictions. In general, the standard deviation tends to be similar between the frequentist and Bayesian approaches for each combination of sigma_squared and alpha.\n",
    "\n",
    "The **mean** represents the average prediction and those values differ slightly between the frequentist and Bayesian approaches for each combination of sigma_squared and alpha.\n",
    "\n",
    "Overall, the differences between the frequentist and Bayesian approaches in terms of standard deviation and mean are relatively small. The Bayesian approach accounts for uncertainty by incorporating the prior distribution over the weight parameters, while the frequentist approach relies solely on the maximum likelihood estimation. However, in this specific scenario, the differences in predictive performance between the two approaches appear to be minimal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
